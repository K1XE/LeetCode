<div align="center">
  <h1 style="margin-bottom: 0;">刷题记录</h1>
  <p>
    📚<a href="https://leetcode.cn/studyplan/top-100-liked/" target="_blank">HOT100</a> | 
    📚<a href="https://programmercarl.com/" target="_blank">代码随想录</a> |
    📚<a href="https://leetcode.cn/studyplan/top-interview-150/" target="_blank">top-interview-150</a>
  </p>
  <p>
    📚<a href="https://hwcoder.top/Manual-Coding-1" target="_blank">Large Language Model Algorithm</a>
  </p>
</div>

<div align="center">

### 📝 [LeetCode 刷题进度](https://hwcoder.top/Manual-Coding-1)

| 难度 | 已完成题数 |
| :--: | :--------: |
| 简单 |    117    |
| 中等 |    241    |
| 困难 |     55     |

---


### 📝 [LLM 手撕经典算法 打卡记录](https://hwcoder.top/Manual-Coding-1)

| 序号 | 篇章                                                      | 算法                                                                                                 |
| ---- | ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| 1    | [Attention](https://hwcoder.top/Manual-Coding-1)            | <ul><li>✔️ Scaled Dot-Product Attention</li><li>✔️ Multi-Head Attention (MHA)</li><li>✔️ Multi-Query Attention (MQA)</li><li>✔️ Grouped Query Attention (GQA)</li><li>⬜ Multi-head Latent Attention (MLA)</li></ul> |
| 2    | [nn](https://hwcoder.top/Manual-Coding-2)             | <ul><li>✔️ Layer Normalization (LN)</li><li>✔️ RMSNorm</li><li>✔️ Batch Normalization (BN)</li><li>✔️ Dropout</li><li>✔️ Backpropagation</li><li>✔️ Gradient Accumulation</li></ul> |
| 3    | [Transformer](https://hwcoder.top/Manual-Coding-3)          | <ul><li>✔️ Token Embedding</li><li>✔️ Positional Embedding</li><li>✔️ Encoder Layer (MHA + FFN)</li><li>⬜ Decoder Layer</li><li>⬜ Stacked Encoder/Decoder</li><li>⬜ 完整 Transformer</li></ul> |
| 4    | [Function](https://hwcoder.top/Manual-Coding-4)             | <ul><li>⬜ MSE</li><li>⬜ CE, BCE, KL, Focal</li><li>⬜ Sigmoid, Tanh</li><li>⬜ ReLU, Leaky ReLU, ELU</li><li>⬜ Swish, GeLU, SwiGLU</li><li>⬜ Softmax</li><li>⬜ PPL, ROUGE, BLEU</li></ul> |
| 5    | [ML](https://hwcoder.top/Manual-Coding-5)             | <ul><li>⬜ 线性回归</li><li>⬜ 逻辑回归</li><li>⬜ Softmax 回归</li><li>⬜ 反向传播</li><li>⬜ SGD 优化器</li></ul> |
| 6    | [RLHF](https://hwcoder.top/Manual-Coding-6)                 | <ul><li>⬜ 广义优势估计 (GAE)</li><li>⬜ PPO Loss & Value Loss</li><li>⬜ DPO Loss</li><li>⬜ GRPO Loss</li><li>⬜ 无偏 KL 散度</li></ul> |
